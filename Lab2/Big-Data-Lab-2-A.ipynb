{"cells":[{"cell_type":"markdown","source":["## This Lab is an example of how to work with data"],"metadata":{}},{"cell_type":"markdown","source":["### Steps\n- 1: Read Data\n- 2: Clean/Prep Data\n- 3: Summarize Data"],"metadata":{}},{"cell_type":"markdown","source":["#### Step 1: Read from Parquet File"],"metadata":{}},{"cell_type":"code","source":["parquetDF = spark.read.parquet(\"/tmp/databricks-df-example.parquet\")\ndisplay(parquetDF)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["#### Step 2: Clean/Prep Data"],"metadata":{}},{"cell_type":"code","source":["#Explode the employees column\nfrom pyspark.sql.functions import explode\n\nexplodeDF = unionDF.select(explode(\"employees\").alias(\"e\"))\nflattenDF = explodeDF.selectExpr(\"e.firstName\", \"e.lastName\", \"e.email\", \"e.salary\")\n\nflattenDF.show()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["#Simple filtering\nfilterDF = flattenDF.filter(flattenDF.firstName == \"xiangrui\").sort(flattenDF.lastName)\ndisplay(filterDF)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["#More filtering\nfrom pyspark.sql.functions import col, asc\n\n# Use `|` instead of `or`\nfilterDF = flattenDF.filter((col(\"firstName\") == \"xiangrui\") | (col(\"firstName\") == \"michael\")).sort(asc(\"lastName\"))\ndisplay(filterDF)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["#More filtering\nwhereDF = flattenDF.where((col(\"firstName\") == \"xiangrui\") | (col(\"firstName\") == \"michael\")).sort(asc(\"lastName\"))\ndisplay(whereDF)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["#Replace the null values\nnonNullDF = flattenDF.fillna(\"--\")\ndisplay(nonNullDF)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["#missing names\nfilterNonNullDF = flattenDF.filter(col(\"firstName\").isNull() | col(\"lastName\").isNull()).sort(\"email\")\ndisplay(filterNonNullDF)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["#### Summarize Data"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import countDistinct\n\ncountDistinctDF = nonNullDF.select(\"firstName\", \"lastName\")\\\n  .groupBy(\"firstName\")\\\n  .agg(countDistinct(\"lastName\").alias(\"distinct_last_names\"))\n\ndisplay(countDistinctDF)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["#Sum the salaries\nsalarySumDF = nonNullDF.agg({\"salary\" : \"sum\"})\ndisplay(salarySumDF)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["#Summary stats for salaries\nnonNullDF.describe(\"salary\").show()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["#### Clean up"],"metadata":{}},{"cell_type":"code","source":["dbutils.fs.rm(\"/tmp/databricks-df-example.parquet\", True)"],"metadata":{},"outputs":[],"execution_count":17}],"metadata":{"name":"Big-Data-Lab-2-A","notebookId":712337004975809},"nbformat":4,"nbformat_minor":0}
